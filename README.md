# Embodied AI (具身智能) ![](https://visitor-badge.glitch.me/badge?page_id=putao537.Awesome-Embodied-AI)

<h4 align="center">Everything about Embodied AI.</h4>

<p align="center">
  <strong><a href="#0">Papers</a></strong> •
  <strong><a href="#1">Tutorials</a></strong> •
  <strong><a href="#2">Workshops</a></strong> •
  <strong><a href="#3">Talks</a></strong> •
  <strong><a href="#4">Scholars</a></strong>
</p>

<p align="center">
  <strong><a href="#5">Surveys</a></strong> •
  <strong><a href="#6">Tasks</a></strong> •
  <strong><a href="#7">Virtual Environments</a></strong> •
  <strong><a href="#8">Datasets & Benchmarks</a></strong>
</p>


<h2 id="0">0. Papers</h2>

**By Date:** [[2022]](Papers/2022.md) [[2021]](Papers/2021.md) [[2020]](Papers/2020.md) [[2019]](Papers/2019.md) [[2018]](Papers/2018.md)    
**By Tasks:** [[Locomotion]](Papers/locomotion.md) [[Visual Navigation]](Papers/visual_navigation.md) [[Object Manipulation]](Papers/object_manipulation.md) [[Rearrangement]](Papers/rearrangement.md)

<h2 id="1">1. Tutorials</h2>

|  **Pub.**  | **Title**                                                    |                          **Links**                           |
| :--------: | :----------------------------------------------------------- | :----------------------------------------------------------: |
| **CVPR'22** | Building and Working in Environments for Embodied AI | [HomePage](https://ai-workshops.github.io/building-and-working-in-environments-for-embodied-ai-cvpr-2022/) |


<h2 id="2">2. Workshops</h2>

|  **Pub.**  | **Title**                                                    |                          **Links**                           |
| :--------: | :----------------------------------------------------------- | :----------------------------------------------------------: |
| **CVPR'22** | Embodied AI Workshop | [HomePage](https://embodied-ai.org/) |
| **CVPR'21** | Embodied AI Workshop | [HomePage](https://embodied-ai.org/cvpr2021) |
| **ICCV'21** | SEAI: Simulation Technology for Embodied AI | [HomePage](https://iccv21-seai.github.io/) |
| **CVPR'20** | Embodied AI Workshop | [HomePage](https://embodied-ai.org/cvpr2020) |


<h2 id="3">3. Talks</h2>

<details>
  <summary> xxx </summary>
  
  ### 2022
|  **Pub.**  | **Title**                                                    |                          **Links**                           |
| :--------: | :----------------------------------------------------------- | :----------------------------------------------------------: |
| **TPAMI** | **[xxx]** xxxx | [PDF](xxx) |

</details>


<h2 id="4">4. Scholars</h2>

<details>
  <summary> xxx </summary>
  
  ### 2022
|  **Pub.**  | **Title**                                                    |                          **Links**                           |
| :--------: | :----------------------------------------------------------- | :----------------------------------------------------------: |
| **TPAMI** | **[xxx]** xxxx | [PDF](xxx) |

</details>


<h2 id="5">5. Surveys</h2>

|  **Pub.**  | **Title**                                                    |                          **Links**                           |
| :--------: | :----------------------------------------------------------- | :----------------------------------------------------------: |
| **T-ETCI'22* | A Survey of Embodied AI: From Simulators to Research Tasks | [PDF]([xxx](https://arxiv.org/abs/2103.04918)) |


<h2 id="6">6. Tasks</h2>

<details>
  <summary> Locomotion </summary>  


</details>

<details>
  <summary> Visual Navigation </summary>  

  - **REVERIE** (From "REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments", CVPR 2020, [PDF](https://arxiv.org/abs/1904.10151))
    REVERIE requires an intelligent agent to correctly localize a remote target object (can not be observed at starting location) specified by a concise high-level natural language instruction.
  
  - **TOUCHDOWN** (From "TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments", CVPR 2019, [PDF](https://arxiv.org/abs/1811.12354))
    TOUCHDOWN requires an agent to first follow navigation instructions in a real-life visual urban environment, and then identify a location described in natural language to find a hidden object at the goal position.

  - **VNLA** (From "Vision-based Navigation with Language-based Assistance via Imitation Learning with Indirect Intervention", CVPR 2019, [PDF](https://arxiv.org/abs/1812.04155))
    VNLA requires an embodied agent to follow natural language instructions to navigate from a starting pose to a goal location. 
  
  - **VLN** (From "Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments", CVPR 2018, [PDF](https://arxiv.org/abs/1711.07280))
    VLN requires an embodied agent to follow natural language instructions to navigate from a starting pose to a goal location.  
  
  - **IQA** (From "IQA: Visual Question Answering in Interactive Environments", CVPR 2018, [PDF](https://arxiv.org/abs/1712.03316)) 
    IQA puts an intelligent agent at random location in a 3D environment and asked a question. This task requires an agent to navigate around the scene, acquire visual understanding of scene elements, interact with objects (e.g. open refrigerators) and plan for a series of actions conditioned on the question.
   
  - **EQA** (From "Embodied Question Answering", CVPR 2018, [PDF](https://arxiv.org/abs/1711.11543))
    EAQ puts an intelligent agent at random location in a 3D environment and asked a question. The agent must first intelligently navigate to explore the environment, gather necessary visual information through first-person (egocentric) vision, and then answer the question.

</details>


<details>
  <summary> Object Manipulation </summary>  


</details>


<details>
  <summary> Rearrangement </summary>  


</details>



<h2 id="7">7. Virtual Environments</h2>

<details>
  <summary> xxx </summary>
  
  ### 2022
|  **Pub.**  | **Title**                                                    |                          **Links**                           |
| :--------: | :----------------------------------------------------------- | :----------------------------------------------------------: |
| **TPAMI** | **[xxx]** xxxx | [PDF](xxx) |

</details>


<h2 id="8">8. Datasets & Benchmarks</h2>

<details>
  <summary> xxx </summary>
  
  ### 2022
|  **Pub.**  | **Title**                                                    |                          **Links**                           |
| :--------: | :----------------------------------------------------------- | :----------------------------------------------------------: |
| **TPAMI** | **[xxx]** xxxx | [PDF](xxx) |

</details>
